{"cells":[{"cell_type":"markdown","id":"BYYL_jnG0B3i","metadata":{"id":"BYYL_jnG0B3i"},"source":["# <font color = 'indianred'> **Logistic Regression using Minibatch Stochastic Gradient Descent with PyTorch**"]},{"cell_type":"markdown","id":"o6jh4FfFPgi_","metadata":{"id":"o6jh4FfFPgi_"},"source":["## <font color = 'indianred'>**Data**"]},{"cell_type":"code","execution_count":null,"id":"1pN2psBZPquN","metadata":{"execution":{"iopub.execute_input":"2022-10-26T19:14:42.405079Z","iopub.status.busy":"2022-10-26T19:14:42.404795Z","iopub.status.idle":"2022-10-26T19:14:42.725729Z","shell.execute_reply":"2022-10-26T19:14:42.725116Z","shell.execute_reply.started":"2022-10-26T19:14:42.405000Z"},"id":"1pN2psBZPquN"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"id":"dHjBoauBQ809","metadata":{"id":"dHjBoauBQ809"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.functional as F\n","from torch.utils.data import DataLoader, TensorDataset, Dataset"]},{"cell_type":"code","execution_count":null,"id":"WUcQ0mpp3HNm","metadata":{"id":"WUcQ0mpp3HNm"},"outputs":[],"source":["X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"rANPOz2gM6jM","metadata":{"id":"rANPOz2gM6jM"},"outputs":[],"source":["preprocessor = StandardScaler()\n","X = preprocessor.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"id":"IKQM7PcKVDkK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1694452376284,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"IKQM7PcKVDkK","outputId":"b0954365-541b-4abe-aa35-cfe6d8d41809"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1000, 2) (1000,)\n"]}],"source":["print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":null,"id":"YhLylG90VHG5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1694452376284,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"YhLylG90VHG5","outputId":"744bed7a-102a-4fea-c8a1-e0db680ced08"},"outputs":[{"data":{"text/plain":["array([[ 1.75081891,  0.48393978],\n","       [ 1.35606443, -0.90706929],\n","       [-0.90150442,  1.22470664],\n","       [-0.60117222, -0.14688373],\n","       [ 0.0048725 , -1.28700543]])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["X[0:5]"]},{"cell_type":"code","execution_count":null,"id":"a8ZlJ168VLQL","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1694452376284,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"a8ZlJ168VLQL","outputId":"003262b7-c8b8-46a9-f2fc-bd39462cff18"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 1 0 1 1 1 0 0 0 1]\n"]}],"source":["print(y[0:10])"]},{"cell_type":"markdown","id":"tUYUlTaEQW2w","metadata":{"id":"tUYUlTaEQW2w"},"source":["## <font color = 'indianred'>**Dataset and Data Loaders**"]},{"cell_type":"code","execution_count":null,"id":"hqQS3ENqNE2l","metadata":{"id":"hqQS3ENqNE2l"},"outputs":[],"source":["# Create Tensors from numpy\n","x_tensor = torch.tensor(X).float()\n","y_tensor = torch.tensor(y.reshape(-1, 1)).float()\n","\n","# create a PyTorch Dataset from x and y tensors\n","train_dataset = TensorDataset(x_tensor, y_tensor)"]},{"cell_type":"code","execution_count":null,"id":"kYxLnEeqoO2j","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1694452376285,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"kYxLnEeqoO2j","outputId":"7182e8ea-9dd0-4cd6-db08-f2de41a6f8ef"},"outputs":[{"data":{"text/plain":["(tensor([1.7508, 0.4839]), tensor([1.]))"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset[0]"]},{"cell_type":"code","execution_count":null,"id":"oUwspygwn1W7","metadata":{"id":"oUwspygwn1W7"},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, X, y):\n","\n","        self.features = X\n","        self.labels = y\n","\n","    def __len__(self):\n","        return self.labels.shape[0]\n","\n","    def __getitem__(self, index):\n","        x = self.features[index]\n","        y = self.labels[index]\n","        return x, y"]},{"cell_type":"code","execution_count":null,"id":"KgOTgyIqo25j","metadata":{"id":"KgOTgyIqo25j"},"outputs":[],"source":["train_dataset2 = MyDataset(x_tensor, y_tensor)"]},{"cell_type":"code","execution_count":null,"id":"BPyHbCk1pNV8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1694452376285,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"BPyHbCk1pNV8","outputId":"c2bee90e-870c-4918-dd63-e8ad8018cc7d"},"outputs":[{"data":{"text/plain":["(tensor([1.7508, 0.4839]), tensor([1.]))"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset2[0]"]},{"cell_type":"code","execution_count":null,"id":"iYkrYFOjnzy7","metadata":{"id":"iYkrYFOjnzy7"},"outputs":[],"source":["# CReate Data lOader from Dataset\n","train_loader = DataLoader(dataset=train_dataset2, batch_size=16, shuffle=True)"]},{"cell_type":"markdown","id":"189Bp-1CRTWb","metadata":{"id":"189Bp-1CRTWb"},"source":["## <font color = 'indianred'>**Loss function**"]},{"cell_type":"markdown","id":"hmetzbpH1Vij","metadata":{"id":"hmetzbpH1Vij"},"source":["**Binary Classification - using Sigmoid**\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1DT7n0Lmbt1hzUvSH9AS9SSydEvOJEPbE\" >"]},{"cell_type":"markdown","id":"aDu7Tr3tsdac","metadata":{"id":"aDu7Tr3tsdac"},"source":["### <font color = 'indianred'>**nn.BCELoss()**"]},{"cell_type":"code","execution_count":null,"id":"YNbzXxyUi6Yu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1694452376285,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"YNbzXxyUi6Yu","outputId":"768f65f8-7541-472c-af09-8a6d62f63e06"},"outputs":[{"name":"stdout","output_type":"stream","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3, 2])\n","\n","output:tensor([[0.6293],\n","        [0.6190],\n","        [0.4345]], grad_fn=<SigmoidBackward0>)\n","\n","loss:0.7539259791374207\n"]}],"source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).view(3,1)\n","print(f'target.shape:{input.shape}\\n')\n","\n","# model - with sigmoid activation for outpur layer\n","model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# BCE Loss function\n","loss_fn = nn.BCELoss()\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"]},{"cell_type":"markdown","id":"e_CFDLsBsrOt","metadata":{"id":"e_CFDLsBsrOt"},"source":["### <font color = 'indianred'>**nn.BCEWithLogitsLoss()**"]},{"cell_type":"code","execution_count":null,"id":"Q7VICYTZjfTp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694452376285,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"Q7VICYTZjfTp","outputId":"009549b8-f54a-4f77-ed88-4ef96f662fc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3, 2])\n","\n","output:tensor([[ 0.5292],\n","        [ 0.4855],\n","        [-0.2635]], grad_fn=<AddmmBackward0>)\n","\n","loss:0.7539259791374207\n"]}],"source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).view(3,1)\n","print(f'target.shape:{input.shape}\\n')\n","\n","# model - without sigmoid activation for outpur layer\n","model = nn.Sequential(nn.Linear(2, 1))\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# BCE with Logistic Loss function\n","loss_fn = nn.BCEWithLogitsLoss()\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"]},{"cell_type":"markdown","id":"W0q0YTWDz232","metadata":{"id":"W0q0YTWDz232"},"source":["<font color= 'indianred' fomnt size = 5>**Both `nn.BCELoss()` and `nn.BCEWithLogitsLoss()` are used for binary classification problems** </font>, but they differ in how they accept input and perform calculations, impacting their numerical stability and performance.\n","\n","### nn.BCELoss():\n","\n","1. **Input Range**: Expects the input tensor to be in the range \\([0, 1]\\), typically the output of a sigmoid activation function.\n","2. **Target**: Binary labels that are either 0 or 1.\n","3. **Standalone**: Applies only the Binary Cross Entropy loss function.\n","\n","### nn.BCEWithLogitsLoss():\n","\n","1. **Input Range**: Accepts raw scores (logits) without any activation function applied.\n","2. **Target**: Binary labels that are either 0 or 1.\n","3. **Combined**: Applies both the sigmoid activation function and the Binary Cross Entropy loss in a single, more numerically stable step.\n","\n","#### Why nn.BCEWithLogitsLoss() is Generally Preferred:\n","\n","1. **Numerical Stability**: Using `nn.BCEWithLogitsLoss()` is more numerically stable than using `nn.BCELoss()` with a separate sigmoid activation. This is because `nn.BCEWithLogitsLoss()` combines the sigmoid activation and the loss calculation into a single operation, which can avoid some of the numerical instability incurred by calculating them separately.\n","\n","2. **Performance**: Combining the sigmoid operation with the loss calculation can lead to optimizations. Backpropagation through a combined operation is often faster than through separate operations.\n","\n","3. **Memory Efficiency**: Since you don't need to store the intermediate sigmoid outputs for backpropagation, using `nn.BCEWithLogitsLoss()` can be more memory-efficient, particularly important for large-scale models.\n","\n","So, for better numerical stability, performance, and memory efficiency, `nn.BCEWithLogitsLoss()` is generally the recommended choice."]},{"cell_type":"markdown","id":"_hJO0-tY2F05","metadata":{"id":"_hJO0-tY2F05"},"source":["<br><br><br><br>\n","\n","**Binary Classification - using Softmax**\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1DVSgL5tEvWRYt4XhlqwEEAhu8FOqt9wK\" >\n","\n"]},{"cell_type":"markdown","id":"uqToyS6LuClY","metadata":{"id":"uqToyS6LuClY"},"source":["### <font color = 'indianred'>**nn.NLLLoss()**"]},{"cell_type":"code","execution_count":null,"id":"2e29b5a2","metadata":{},"outputs":[],"source":["# Minimizing the negative log-likelihood is equivalent to maximizing the likelihood."]},{"cell_type":"code","execution_count":null,"id":"-7UaZ45Ep2cT","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1694452376286,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"-7UaZ45Ep2cT","outputId":"36ba7c3d-c181-4c22-bc46-658af5edb404"},"outputs":[{"name":"stdout","output_type":"stream","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3, 2])\n","\n","output:tensor([[-0.4640, -0.9909],\n","        [-0.4635, -0.9917],\n","        [-0.5980, -0.7984]], grad_fn=<LogSoftmaxBackward0>)\n","\n","loss:0.7509231567382812\n"]}],"source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).long()\n","print(f'target.shape:{input.shape}\\n')\n","\n","# model - without sigmoid activation for outpur layer\n","model = nn.Sequential(nn.Linear(2, 2), nn.LogSoftmax(dim =1))\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# Negative Log Likelihood Function\n","loss_fn = nn.NLLLoss()\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"]},{"cell_type":"markdown","id":"LpMT910VuNvG","metadata":{"id":"LpMT910VuNvG"},"source":["### <font color = 'indianred'>**nn.CrossEntropy()**"]},{"cell_type":"code","execution_count":null,"id":"YE8XXy2-mzLN","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694452376286,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"YE8XXy2-mzLN","outputId":"80e32e95-c6f9-4c3d-ae83-f9b309e34942"},"outputs":[{"name":"stdout","output_type":"stream","text":["input.shape:torch.Size([3, 2])\n","\n","target.shape:torch.Size([3])\n","\n","output:tensor([[ 0.7333,  0.2065],\n","        [ 0.6896,  0.1615],\n","        [-0.0593, -0.2597]], grad_fn=<AddmmBackward0>)\n","\n","loss:0.7509231567382812\n"]}],"source":["torch.manual_seed(42)\n","input = torch.randn(6, requires_grad=True).view(3,2)\n","print(f'input.shape:{input.shape}\\n')\n","target = torch.tensor([1.0, 0., 1.0]).long()\n","print(f'target.shape:{target.shape}\\n')\n","\n","# model - without sigmoid activation for outpur layer\n","model = nn.Sequential(nn.Linear(2, 2))\n","output = model(input)\n","print(f'output:{output}\\n')\n","\n","# CrossEntropy Function\n","loss_fn = nn.CrossEntropyLoss()\n","loss = loss_fn(output, target)\n","print(f'loss:{loss}')"]},{"cell_type":"markdown","id":"0_YFtoAz0rZQ","metadata":{"id":"0_YFtoAz0rZQ"},"source":["<font color= 'indianred' fomnt size = 5>**Both `nn.NLLLoss()` and `nn.CrossEntropyLoss()` are used for classification tasks** </font>, but they operate on different kinds of inputs and perform different computations. Here's a breakdown:\n","\n","### nn.NLLLoss():\n","\n","1. **Input**: Expects log probabilities, typically the output of a `log_softmax` function.\n","2. **Target**: Requires class labels as integers.\n","3. **Operation**: Applies the Negative Log Likelihood (NLL) loss, essentially indexing into the log probabilities based on target labels and negating the values.\n","\n","### nn.CrossEntropyLoss():\n","\n","1. **Input**: Expects raw scores (logits), without any activation function applied.\n","2. **Target**: Requires class labels as integers.\n","3. **Operation**: Combines both the `log_softmax` and `nn.NLLLoss()` in a single operation, making it a more convenient and numerically stable option.\n","\n","#### Why `nn.CrossEntropyLoss()` is Generally Preferred:\n","\n","1. **Numerical Stability**: Similar to the advantage of `nn.BCEWithLogitsLoss()` over `nn.BCELoss()`, `nn.CrossEntropyLoss()` improves numerical stability by combining `log_softmax` and `nn.NLLLoss()` into a single operation.\n","\n","2. **Code Simplification**: You don't have to explicitly include a softmax or log_softmax layer before the loss layer, making the code simpler and less prone to errors.\n","\n","3. **Performance**: The combined operation can result in a performance gain during both forward and backward passes, as some computations can be merged.\n","\n","4. **Memory Efficiency**: No need to store intermediate values from the softmax operation when using `nn.CrossEntropyLoss()`, potentially saving memory.\n","\n","In summary, `nn.CrossEntropyLoss()` is often the more convenient and numerically stable option for classification tasks. It encapsulates the functionalities of `log_softmax` and `nn.NLLLoss()` in a single class, offering advantages in terms of code clarity, performance, and memory efficiency."]},{"cell_type":"markdown","id":"XGTcPVobuxWC","metadata":{"id":"XGTcPVobuxWC"},"source":["### <font color = 'indianred'>**Summary Loss Functions**"]},{"cell_type":"markdown","id":"IUQL8DEDu5c8","metadata":{"id":"IUQL8DEDu5c8"},"source":["<font size = 10></font>\n","\n","|                              | <font size = 4> BCE Loss| <font size = 4> BCE With Logits Loss| <font size = 4> NLL Loss| <font size = 4> Cross Entropy Loss|\n","|--------------------------|-------------|-----------|---------|-----------|\n","|<font size = 4> Classification|<font size = 4> binary| <font size = 4> binary| <font size = 4> multiclass|<font size = 4> multiclass|\n","|<font size = 4> Input (each datapoint)| <font size = 4> probbaility|<font size = 4> logit|<font size = 4> array of log probbailities|<font size = 4> array of logits|\n","<font size = 4> Label (Each data point)| <font size = 4> float (0.0 or 1.0)|<font size = 4> float (0.0 or 1.0)|<font size = 4> long(class index)|<font size = 4> long(class index)|\n","|<font size = 4> Model's Last layer| <font size = 4> Sigmoid | - | <font size = 4> LogSoftmax|-"]},{"cell_type":"markdown","id":"N1rmjk6hyfS0","metadata":{"id":"N1rmjk6hyfS0"},"source":["### <font color = 'indianred'>**Summary Multiclass Classification**\n","\n","<img src = \"https://drive.google.com/uc?export=view&id=1IoVhG4hUUzc_v3oecYQ0r6oZVS7Rhbpz\" width =800 >\n","\n","\n","- **Cross Entropy Loss Function (assuming a batch size of two and four classes (0,1,2,3)**:\n","\\begin{equation}\n","\\mathcal{L} = -\\frac{1}{m} \\sum_{k=1}^{K} \\sum_{i=1}^{m} \\bigg[y_k^{(i)}log(\\hat{p_k}^{(i)}) \\bigg]\n","\\end{equation}\n","\n","<img src = \"https://drive.google.com/uc?export=view&id=1YFkNGN_x1lETidVDNLaSZ4ndwZv9Wc2T\" width =600 >\n","\n","\n","if we use <font color = 'indianred'>**`nn.CrossEntropyLoss()`**\n","- We do not need <font color = 'indianred'> **one-hot encoding**</font> step as PyTorch does this internally.\n","\n","- We do not need the <font color = 'indianred'>**softmax step** </font> as PyTorch does this internally"]},{"cell_type":"markdown","id":"kiWKeaBdx9V_","metadata":{"id":"kiWKeaBdx9V_"},"source":["### <font color = 'indianred'>**Recommendation**\n","- **nn.BCEWithLogitsLoss() for binary classification**\n","- **nn.CrossEntropyLoss() for multi-class classification**\n","\n"]},{"cell_type":"markdown","id":"y3HFDKy1RLMj","metadata":{"id":"y3HFDKy1RLMj"},"source":["## <font color = 'indianred'>**Model**"]},{"cell_type":"code","execution_count":null,"id":"mtFhXfXpROP3","metadata":{"id":"mtFhXfXpROP3"},"outputs":[],"source":["# Speciy your model\n","model = nn.Sequential(nn.Linear(2, 1))"]},{"cell_type":"markdown","id":"bDkkl8M2c7xo","metadata":{"id":"bDkkl8M2c7xo"},"source":["## <font color = 'indianred'>**Loss Function**"]},{"cell_type":"code","execution_count":null,"id":"ZyxG9cEIRdxD","metadata":{"id":"ZyxG9cEIRdxD"},"outputs":[],"source":["# specify your loss function\n","loss_function = nn.BCEWithLogitsLoss(reduction ='mean')"]},{"cell_type":"markdown","id":"Mu2DOkq5SH_X","metadata":{"id":"Mu2DOkq5SH_X"},"source":["## <font color = 'indianred'>**Optimizer**"]},{"cell_type":"code","execution_count":null,"id":"z2ZPyfdlSMi9","metadata":{"id":"z2ZPyfdlSMi9"},"outputs":[],"source":["# specify optimizer\n","learning_rate = 0.1\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate )"]},{"cell_type":"markdown","id":"Z0ETm1C0apXS","metadata":{"id":"Z0ETm1C0apXS"},"source":["## <font color = 'indianred'>**Initialization**\n","\n","Create a function to initilaize weights.\n","- Initialize weights using normal distribution with mean = 0 and std = 0.05\n","- Initilaize the bias term with zeros"]},{"cell_type":"code","execution_count":null,"id":"P7_uk-Wya5w-","metadata":{"id":"P7_uk-Wya5w-"},"outputs":[],"source":["def init_weights(layer):\n","  if type(layer) == nn.Linear:\n","    torch.nn.init.normal_(layer.weight, mean = 0, std = 0.05)\n","    torch.nn.init.zeros_(layer.bias)"]},{"cell_type":"markdown","id":"_s37DSbzSkWF","metadata":{"id":"_s37DSbzSkWF"},"source":["## <font color = 'indianred'>**Training Loop**"]},{"cell_type":"markdown","id":"0m8CNyv7Syxx","metadata":{"id":"0m8CNyv7Syxx"},"source":["**Model Training** involves five steps:\n","\n","- Step 0: Randomly initialize parameters / weights\n","- Step 1: Compute model's predictions - forward pass\n","- Step 2: Compute loss\n","- Step 3: Compute the gradients\n","- Step 4: Update the parameters\n","- Step 5: Repeat steps 1 - 4\n","\n","Model training is repeating this process over and over, for many **epochs**.\n","\n","We will specify number of ***epochs*** and during each epoch we will iterate over the complete dataset and will keep on updating the parameters.\n","\n","***Learning rate*** and ***epochs*** are known as hyperparameters. We have to adjust the values of these two based on validation dataset.\n","\n","We will now create functions for step 1 to 4."]},{"cell_type":"code","execution_count":null,"id":"zRFZkBl-TGPC","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8643,"status":"ok","timestamp":1694452385224,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"zRFZkBl-TGPC","outputId":"b336c7f2-1720-4930-ac41-29998dd11b37"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n","Epoch : 1 / 5\n","Train Loss:  0.4049 | Train Accuracy:  86.3000%\n","Epoch : 2 / 5\n","Train Loss:  0.2908 | Train Accuracy:  86.6000%\n","Epoch : 3 / 5\n","Train Loss:  0.2648 | Train Accuracy:  87.0000%\n","Epoch : 4 / 5\n","Train Loss:  0.2562 | Train Accuracy:  87.5000%\n","Epoch : 5 / 5\n","Train Loss:  0.2502 | Train Accuracy:  87.8000%\n"]}],"source":["torch.manual_seed(100)\n","epochs = 5\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","# move model tp gpu\n","model.to(device)\n","\n","# Step 0: Randomly initialize parameters / weights\n","\n","model.apply(init_weights)\n","\n","for epoch in range(epochs):\n","\n","  # Training Loop\n","  # Initialize train_loss at the he start of the epoch\n","  running_train_loss = 0\n","  running_train_correct = 0\n","\n","  # Iterate on batches from the dataset using train_loader\n","  for input_, targets in train_loader:\n","\n","    # move inputs and outputs to GPUs\n","    input_ = input_.to(device)\n","    targets = targets.to(device)\n","\n","    # Step 1: Forward Pass: Compute model's predictions\n","    output = model(input_)\n","\n","    # Step 2: Compute loss\n","    loss = loss_function(output, targets)\n","\n","    # Step 3: Backward pass -Compute the gradients\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    # Step 4: Update the parameters\n","    optimizer.step()\n","\n","    # Add train loss of a batch\n","    running_train_loss += loss.item()\n","\n","    with torch.no_grad():\n","      # Correct prediction\n","      y_pred = output>=0 # get the predicted class\n","      correct = torch.sum(y_pred == targets)\n","      # Add Corect counts of a batch\n","      running_train_correct += correct\n","\n","  # Calculate mean train loss for the whole dataset for a particular epoch\n","  train_loss = running_train_loss/len(train_loader)\n","\n","  # Calculate accuracy for the whole dataset for a particular epoch\n","  train_acc = running_train_correct/len(train_loader.dataset)\n","\n","  # Print the train loss and accuracy for given number of epochs, batch size and number of samples\n","  print(f'Epoch : {epoch+1} / {epochs}')\n","  print(f'Train Loss: {train_loss : .4f} | Train Accuracy: {train_acc * 100 : .4f}%')"]},{"cell_type":"code","execution_count":null,"id":"6V3o_7WCYWac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1694452385225,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"6V3o_7WCYWac","outputId":"6c472362-9435-4e20-f4e9-926ee3130f6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.weight tensor([[ 0.9944, -2.3283]], device='cuda:0')\n","0.bias tensor([-0.0195], device='cuda:0')\n"]}],"source":["# print the estimated weight and bias term\n","for name, param in model.named_parameters():\n","  print(name, param.data)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}
